# Awesome-Video-Reasoning-Landscape [![Awesome](https://awesome.re/badge.svg)](https://awesome.re)

<!-- markdownlint-disable MD033 -->
## The Landscape of Video Reasoning: Tasks, Paradigms and Benchmarks‚Äî An Open-Source Survey

## Overview
This Awesome list systematically curates and tracks the latest progress in **Video Reasoning**, covering diverse modalities, tasks, and modeling paradigms. Rather than focusing on a single line of research, we organize the landscape from multiple complementary perspectives. Following the emerging taxonomy of the field, current works are grouped into four major paradigms:

- üóíÔ∏è **CoT-based Video Reasoning** ‚Äî language-centric, chain-of-thought reasoning with Video-LMMs  
- üïπÔ∏è **CoF-based Video Reasoning** ‚Äî vision-centric reasoning grounded in world models or video generation  
- üåà **Interleaved Video Reasoning** ‚Äî unified models that integrate multimodal interaction and iterative inference  
- üîÅ **Streaming Video Reasoning** ‚Äî continuous, low-latency reasoning over long or unbounded video streams with online perception and incremental state updates.
  
We additionally maintain a dedicated **Benchmark** section that summarizes datasets, evaluation settings, and standardized tasks to support fair comparison across paradigms.

This repository aims to provide a structured, up-to-date, and open-source overview of the evolving landscape of video reasoning.  
**Contributions and PRs are warmly welcome ‚Äî preferably in reverse chronological order (newest first)** to keep the list fresh and easy to browse.


## Table of Contents
- [Awesome-Video-Reasoning-Landscape](#awesome-video-reasoning-landscape-)
  - [üìë Task Definition](#-task-definition)
  - [üòé Paradigms](#-paradigms)
    - [üóíÔ∏è CoT-based Video Reasoning](#Ô∏è-cot-based-video-reasoning)
    - [üïπÔ∏è CoF-based Video Reasoning](#Ô∏è-cof-based-video-reasoning)
    - [üåà Interleaved Video Reasoning](#-interleaved-video-reasoning)
    - [üîÅ Streaming Video Reasoning](#-streaming-video-reasoning)
  - [‚ú®Ô∏è Benchmarks](#-benchmarks)
  - [‚úà Related Surveys](#-related-survey)
  - [üåü Star History](#-star-history)
  - [‚ô•Ô∏è Contributors](#Ô∏è-contributors)

<!-- <small><i><a href='http://eCoTrust-canada.github.io/markdown-toc/'>`Table of contents generated with markdown-toc`</a></i></small> -->

## üìë Task Definition
TBD

## üòé Paradigms

<!-- Á¨¶Âè∑:
‚àö ‚úì
x ‚úó
         arxiv: https://img.shields.io/badge/arXiv-2410.12109-b31b1b.svg?style=plastic
         conference: https://img.shields.io/badge/CVPR-2024-blue.svg?style=plastic
         huggingface checkpoint:![hf_checkpoint](https://img.shields.io/badge/ü§ó-Checkpoints-9C276A.svg)]()
         modelscope
         github model zoos: [![github_model_zoos](https://img.shields.io/badge/ModelZoo-black?logo=github)]()
-->

<!-- Ê®°ÁâàÔºö
|** ** [![arXiv](https://img.shields.io/badge/arXiv-[]-b31b1b.svg?style=plastic)](https://arxiv.org/abs/[])|** ** [![](https://img.shields.io/badge/Github-181717?style=plastic&logo=github&logoColor=white)](https://om-cat.github.io.)|unreleased|‚úì|‚úì|‚úì|‚úì|
[![arXiv](https://img.shields.io/badge/arXiv-2505.02064-b31b1b.svg?style=plastic)](https://arxiv.org/abs/2505.02064)
 
 -->

 
 ### üïπÔ∏è CoT-based Video Reasoning

| **Title** | **Model & Code** | **Checkpoint** | **Input Modalities** | **Time** | **Venue** |
|:----------|:-------------------:|:----------------:|:----------------------:|:----------:|:-----------:|
| [Rethinking Chain-of-Thought Reasoning for Videos](https://arxiv.org/abs/2512.09616) | [GitHub](https://github.com/LaVi-Lab/Rethink_CoT_Video) ![](https://img.shields.io/github/stars/LaVi-Lab/Rethink_CoT_Video?style=social) |`N/A`|`Text` `Video`|2025-12|[![arXiv](https://img.shields.io/badge/arXiv-2512.09616-b31b1b.svg?style=plastic)](https://arxiv.org/abs/2512.09616)|
| [1+1 > 2 : Detector-Empowered Video Large Language Model for Spatio-Temporal Grounding and Reasoning](https://arxiv.org/abs/2512.06673) | [GitHub](https://github.com/gaostar123/DeViL) ![](https://img.shields.io/github/stars/gaostar123/DeViL?style=social) |`N/A`|`Text` `Video`|2025-12|[![arXiv](https://img.shields.io/badge/arXiv-2512.06673-b31b1b.svg?style=plastic)](https://arxiv.org/abs/2512.06673)|
| [TempR1: Improving Temporal Understanding of MLLMs via Temporal-Aware Multi-Task Reinforcement Learning](https://arxiv.org/pdf/2512.03963) | | | `Text` `Video` |2025-12|[![arXiv](https://img.shields.io/badge/arXiv-2512.03963-b31b1b.svg?style=plastic)](https://arxiv.org/pdf/2512.03963)|
| [OneThinker: All-in-one Reasoning Model for Image and Video](https://arxiv.org/abs/2512.03043) | [GitHub](https://github.com/tulerfeng/OneThinker) ![](https://img.shields.io/github/stars/tulerfeng/OneThinker?style=social) |[Hugging_Face](https://huggingface.co/OneThink)|`Text` `Video`|2025-12|[![arXiv](https://img.shields.io/badge/arXiv-2512.03043-b31b1b.svg?style=plastic)](https://arxiv.org/abs/2512.03043)|
| [WorldMM: Dynamic Multimodal Memory Agent for Long Video Reasoning](https://arxiv.org/abs/2512.02425) | [GitHub](https://github.com/wgcyeo/WorldMM) ![](https://img.shields.io/github/stars/wgcyeo/WorldMM?style=social) |-|`Text` `Video`|2025-12|[![arXiv](https://img.shields.io/badge/arXiv-2512.02425-b31b1b.svg?style=plastic)](https://arxiv.org/abs/2512.02425)
| [Thinking with Drafts: Speculative Temporal Reasoning for Efficient Long Video Understanding](https://arxiv.org/abs/2512.00805) |- |-|`Text` `Video`|2025-12|[![arXiv](https://img.shields.io/badge/arXiv-2512.00805-b31b1b.svg?style=plastic)](https://arxiv.org/abs/2512.00805)
| [Video-R2: Reinforcing Consistent and Grounded Reasoning in Multimodal Language Models](https://arxiv.org/abs/2511.23478) | [GitHub](https://github.com/mbzuai-oryx/Video-R2) ![](https://img.shields.io/github/stars/mbzuai-oryx/Video-R2?style=social) | - |`Text` `Video` | 2025-11| [![arXiv](https://img.shields.io/badge/arXiv-2511.23478-b31b1b.svg?style=plastic)](https://arxiv.org/abs/2511.23478)
| [Video-CoM: Interactive Video Reasoning via Chain of Manipulations](https://arxiv.org/abs/2511.23477) | [GitHub](https://github.com/mbzuai-oryx/Video-CoM) ![](https://img.shields.io/github/stars/mbzuai-oryx/Video-CoM?style=social) | - |`Text` `Video`  | 2025-11 |[![arXiv](https://img.shields.io/badge/arXiv-2511.23477-b31b1b.svg?style=plastic)](https://arxiv.org/abs/2511.23477)|
| [VideoSeg-R1: Reasoning Video Object Segmentation via Reinforcement Learning](https://arxiv.org/abs/2511.16077)| [GitHub](https://github.com/euyis1019/VideoSeg-R1) ![](https://img.shields.io/github/stars/euyis1019/VideoSeg-R1?style=social)| - | `Text` `Video` | 2025-11 |[![arXiv](https://img.shields.io/badge/arXiv-2511.16077-b31b1b.svg?style=plastic)](https://arxiv.org/abs/2511.16077)|
| [AVATAAR: Agentic Video Answering via Temporal Adaptive Alignment and Reasoning](https://arxiv.org/abs/2511.15578)| `N/A` | `N/A` | `Audio` `Video` | 2025-11 |[![arXiv](https://img.shields.io/badge/arXiv-2511.15578-b31b1b.svg?style=plastic)](https://arxiv.org/abs/2511.15578)|
| [Agentic Video Intelligence: A Flexible Framework for Advanced Video Exploration and Understanding](https://arxiv.org/abs/2511.14446) | - | - | `Text` `Video` | 2025-11 |[![arXiv](https://img.shields.io/badge/arXiv-2511.14446-b31b1b.svg?style=plastic)](https://arxiv.org/abs/2511.14446)|
| [Video Spatial Reasoning with Object-Centric 3D Rollout](https://arxiv.org/abs/2511.13190)| - | - |`Text`  `Video` | 2025-11 |[![arXiv](https://img.shields.io/badge/arXiv-2511.13190-b31b1b.svg?style=plastic)](https://arxiv.org/abs/2511.13190)|
| [ViSS-R1: Self-Supervised Reinforcement Video Reasoning](https://arxiv.org/abs/2511.13054) | - | - | `Text` `Video` | 2025-11 |[![arXiv](https://img.shields.io/badge/arXiv-2511.13054-b31b1b.svg?style=plastic)](https://arxiv.org/abs/2511.13054)|
| [Video-Thinker: Sparking "Thinking with Videos" via Reinforcement Learning](https://arxiv.org/abs/2510.23473)| [GitHub](https://github.com/shijian2001/Video-Thinker) ![](https://img.shields.io/github/stars/shijian2001/Video-Thinker?style=social)| [Hugging_Face](https://huggingface.co/ShijianW01/Video-Thinker-7B) | `Text` `Video` | 2025-10 | [![arXiv](https://img.shields.io/badge/arXiv-2510.23473-b31b1b.svg?style=plastic)](https://arxiv.org/abs/2510.23473) |
| [Open-o3 Video: Grounded Video Reasoning with Explicit Spatio-Temporal Evidence](https://arxiv.org/pdf/2510.20579)| [GitHub](https://github.com/marinero4972/Open-o3-Video) ![](https://img.shields.io/github/stars/marinero4972/Open-o3-Video?style=social)| [Hugging_Face](https://huggingface.co/marinero4972/Open-o3-Video) | `Text` `Video` | 2025-10 | [![arXiv](https://img.shields.io/badge/arXiv-2510.20579-b31b1b.svg?style=plastic)](https://arxiv.org/abs/2510.20579) |
| [VideoChat-R1.5: Visual Test-Time Scaling to Reinforce Multimodal Reasoning by Iterative Perception](https://arxiv.org/abs/2509.21100) | [GitHub](https://github.com/OpenGVLab/VideoChat-R1) ![](https://img.shields.io/github/stars/OpenGVLab/VideoChat-R1?style=social)| [Hugging_Face](https://huggingface.co/OpenGVLab/VideoChat-R1_5-7B) | `Text` `Video` | 2025-09 | [![arXiv](https://img.shields.io/badge/arXiv-2509.21100-b31b1b.svg?style=plastic)](https://arxiv.org/abs/2509.21100) |
| [MOSS-ChatV: Reinforcement Learning with Process Reasoning Reward for Video Temporal Reasoning](https://arxiv.org/abs/2509.21113) | - | - | `Text` `Video` | 2025-09 | [![arXiv](https://img.shields.io/badge/arXiv-2509.21113-b31b1b.svg?style=plastic)](https://arxiv.org/abs/2509.21113) |
| [Kwai Keye-VL 1.5 Technical Report](https://arxiv.org/abs/2509.01563) | [GitHub](https://github.com/Kwai-Keye/Keye) ![](https://img.shields.io/github/stars/Kwai-Keye/Keye?style=social)| [Hugging_Face](https://huggingface.co/Kwai-Keye)  | `Text` `Video` | 2025-09 | [![arXiv](https://img.shields.io/badge/arXiv-2509.01563-b31b1b.svg?style=plastic)](https://arxiv.org/abs/2509.01563) |
| [Strefer: Empowering Video LLMs with Space-Time Referring and Reasoning via Synthetic Instruction Data](https://arxiv.org/abs/2509.03501) | [GitHub](https://github.com/SalesforceAIResearch/strefer) ![](https://img.shields.io/github/stars/SalesforceAIResearch/strefer?style=social)| [Google_Drive](https://drive.google.com/file/d/1hUa_A_qp7stsDhrRuD7_4NotAs6PW2gz/view?usp=sharing) | `Text` `Video` | 2025-09 | [![arXiv](https://img.shields.io/badge/arXiv-2509.03501-b31b1b.svg?style=plastic)](https://arxiv.org/abs/2509.03501) |
| [Video-MTR: Reinforced Multi-Turn Reasoning for Long Video Understanding](https://arxiv.org/abs/2508.20478) | - | - | `Text` `Video` | 2025-08 | [![arXiv](https://img.shields.io/badge/arXiv-2508.20478-b31b1b.svg?style=plastic)](https://arxiv.org/abs/2508.20478) |
| [Ovis2.5 Technical Report](https://arxiv.org/abs/2508.11737) |  [GitHub](https://github.com/AIDC-AI/Ovis) ![](https://img.shields.io/github/stars/AIDC-AI/Ovis?style=social) | [Hugging_Face](https://huggingface.co/collections/AIDC-AI/ovis25) | `Text` `Video` | 2025-08 | [![arXiv](https://img.shields.io/badge/arXiv-2508.11737-b31b1b.svg?style=plastic)](https://arxiv.org/abs/2508.11737) |
| [Veason-R1: Reinforcing Video Reasoning Segmentation to Think Before It Segments](https://arxiv.org/abs/2508.11538) | - | - | `Text` `Video` | 2025-08 | [![arXiv](https://img.shields.io/badge/arXiv-2407.05513-b31b1b.svg?style=plastic)](https://arxiv.org/abs/2508.11538) |
| [ReasoningTrack: Chain-of-Thought Reasoning for Long-term Vision-Language Tracking](https://arxiv.org/abs/2508.05221) |  [GitHub](https://github.com/Event-AHU/Open_VLTrack) ![](https://img.shields.io/github/stars/Event-AHU/Open_VLTrack?style=social) | - | `Text` `Video` | 2025-08 | [![arXiv](https://img.shields.io/badge/arXiv-2508.05221-b31b1b.svg?style=plastic)](https://arxiv.org/abs/2508.05221) |
| [TAR-TVG: Enhancing VLMs with Timestamp Anchor-Constrained Reasoning for Temporal Video Grounding](https://arxiv.org/abs/2508.07683) | - | - | `Text` `Video` | 2025-08 | [![arXiv](https://img.shields.io/badge/arXiv-2508.07683-b31b1b.svg?style=plastic)](https://arxiv.org/abs/2508.07683) |
| [Thinking With Videos: Multimodal Tool-Augmented Reinforcement Learning for Long Video Reasoning](https://arxiv.org/abs/2508.04416) |  [GitHub](https://github.com/zhang9302002/ThinkingWithVideos) ![](https://img.shields.io/github/stars/zhang9302002/ThinkingWithVideos?style=social) | [Hugging_Face](https://huggingface.co/datasets/zhang9302002/MultiTaskVideoReasoning) | `Text` `Video` | 2025-08 | [![arXiv](https://img.shields.io/badge/arXiv-2508.04416-b31b1b.svg?style=plastic)](https://arxiv.org/abs/2508.04416) |
| [AVATAR: Reinforcement Learning to See, Hear, and Reason Over Video](https://arxiv.org/abs/2508.03100) |  [GitHub](https://github.com/yogkul2000/AVATAR) ![](https://img.shields.io/github/stars/yogkul2000/AVATAR?style=social) | [Hugging_Face](https://huggingface.co/yogkul2000/AVATAR) | `Audio` `Video` `Text` | 2025-08 | [![arXiv](https://img.shields.io/badge/arXiv-2508.03100-b31b1b.svg?style=plastic)](https://arxiv.org/abs/2508.03100) |
| [ReasonAct: Progressive Training for Fine-Grained Video Reasoning in Small Models](https://arxiv.org/abs/2508.01533) | - | - | `Text` `Video` | 2025-08 | [![arXiv](https://img.shields.io/badge/arXiv-2508.01533-b31b1b.svg?style=plastic)](https://arxiv.org/abs/2508.01533) |
| [VideoForest: Person-Anchored Hierarchical Reasoning for Cross-Video Question Answering](https://arxiv.org/abs/2508.03039) | - | - | `Text` `Video` | 2025-08 |  [![](https://img.shields.io/badge/ACM--MM-2025-blue.svg?style=plastic)](https://dl.acm.org/doi/abs/10.1145/3746027.3754573) |
| [ARC-Hunyuan-Video-7B: Structured Video Comprehension of Real-World Shorts](https://arxiv.org/abs/2507.20939) |  [GitHub](https://github.com/TencentARC/ARC-Hunyuan-Video-7B) ![](https://img.shields.io/github/stars/TencentARC/ARC-Hunyuan-Video-7B?style=social) | [Hugging_Face](https://huggingface.co/TencentARC/ARC-Hunyuan-Video-7B) | `Text` `Audio` `Video` | 2025-07 | [![arXiv](https://img.shields.io/badge/arXiv-2507.20939-b31b1b.svg?style=plastic)](https://arxiv.org/abs/2507.20939) |
| [METER: Multi-modal Evidence-based Thinking and Explainable Reasoning -- Algorithm and Benchmark](https://arxiv.org/abs/2507.16206) | - | - | `Text` `Audio` `Video` | 2025-07 | [![arXiv](https://img.shields.io/badge/arXiv-2507.16206-b31b1b.svg?style=plastic)](https://arxiv.org/abs/2507.16206) |
| [CoTasks: Chain-of-Thought based Video Instruction Tuning Tasks](https://arxiv.org/abs/2507.13609) | - | - | `Text` `Video` | 2025-07 | [![arXiv](https://img.shields.io/badge/arXiv-2507.13609-b31b1b.svg?style=plastic)](https://arxiv.org/abs/2507.13609) |
| [EmbRACE-3K: Embodied Reasoning and Action in Complex Environments](https://arxiv.org/abs/2507.10548) | - | - | `Text` `Video` | 2025-07 | [![arXiv](https://img.shields.io/badge/arXiv-2507.10548-b31b1b.svg?style=plastic)](https://arxiv.org/abs/2507.10548) |
| [Scaling RL to Long Videos](https://arxiv.org/abs/2507.07966) | [GitHub](https://github.com/NVlabs/Long-RL) ![](https://img.shields.io/github/stars/NVlabs/Long-RL?style=social) | [Hugging_Face](https://huggingface.co/datasets/LongVideo-Reason/longvideo-reason) | `Text` `Video` | 2025-07 |  [![](https://img.shields.io/badge/NeurIPS-2025-blue.svg?style=plastic)]()  |
| [Kwai Keye-VL Technical Report](https://arxiv.org/abs/2507.01949) |  [GitHub](https://github.com/Kwai-Keye/Keye) ![](https://img.shields.io/github/stars/Kwai-Keye/Keye?style=social) | - | `Text` `Video` | 2025-07 | [![arXiv](https://img.shields.io/badge/arXiv-2507.01949-b31b1b.svg?style=plastic)](https://arxiv.org/abs/2507.01949) |
| [ViTCoT: Video-Text Interleaved Chain-of-Thought for Boosting Video Understanding in Large Language Models](https://arxiv.org/abs/2507.09876) |  [GitHub](https://github.com/BRZ911/ViTCoT) ![](https://img.shields.io/github/stars/BRZ911/ViTCoT?style=social) | - | `Text` `Video` | 2025-07 |  [![](https://img.shields.io/badge/ACM--MM-2025-blue.svg?style=plastic)](https://dl.acm.org/doi/10.1145/3746027.3755837) |
| [Video-RTS: Rethinking Reinforcement Learning and Test-Time Scaling for Efficient and Enhanced Video Reasoning](https://arxiv.org/abs/2507.06485) |  [GitHub](https://github.com/Ziyang412/Video-RTS) ![](https://img.shields.io/github/stars/Ziyang412/Video-RTS?style=social) | [Hugging_Face](https://huggingface.co/Ted412/Video-RTS) | `Text` `Video` | 2025-07 |  [![](https://img.shields.io/badge/EMNLP-2025(Main)-blue.svg?style=plastic)]()|
| [Temporal Chain of Thought: Long-Video Understanding by Thinking in Frames](https://arxiv.org/abs/2507.02001) | - | - | `Text` `Video` | 2025-07 | [![arXiv](https://img.shields.io/badge/arXiv-2507.02001-b31b1b.svg?style=plastic)](https://arxiv.org/abs/2507.02001) |
| [VLN-R1: Vision-Language Navigation via Reinforcement Fine-Tuning](https://arxiv.org/abs/2506.17221) | - | - | `Text` `Video` | 2025-06 | [![arXiv](https://img.shields.io/badge/arXiv-2506.17221-b31b1b.svg?style=plastic)](https://arxiv.org/abs/2506.17221) |
| [Ego-R1: Chain-of-Tool-Thought for Ultra-Long Egocentric Video Reasoning](https://arxiv.org/abs/2506.13654) | [GitHub](https://github.com/egolife-ai/Ego-R1) ![](https://img.shields.io/github/stars/egolife-ai/Ego-R1?style=social) | - | `Text` `Video` | 2025-06 |  [![arXiv](https://img.shields.io/badge/arXiv-2506.13654-b31b1b.svg?style=plastic)](https://arxiv.org/abs/2506.13654)|
| [DAVID-XR1: Detecting AI-Generated Videos with Explainable Reasoning](https://arxiv.org/abs/2506.14827) | - | - | `Text` `Video` | 2025-06 | [![arXiv](https://img.shields.io/badge/arXiv-2506.14827-b31b1b.svg?style=plastic)](https://arxiv.org/abs/2506.14827) |
| [VidBridge-R1: Bridging QA and Captioning for RL-based Video Understanding Models with Intermediate Proxy Tasks](https://arxiv.org/abs/2506.09079) | [GitHub](https://github.com/VidBridge-R1/VidBridge-R1) ![](https://img.shields.io/github/stars/VidBridge-R1/VidBridge-R1?style=social) | [Hugging_Face](https://huggingface.co/datasets/VidBridge-R1/VidBridge-R1_training_data) | `Text` `Video` | 2025-06 | [![arXiv](https://img.shields.io/badge/arXiv-2506.09079-b31b1b.svg?style=plastic)](https://arxiv.org/abs/2506.09079) |
| [MiMo-VL Technical Report](https://arxiv.org/abs/2506.03569) | [GitHub](https://github.com/XiaomiMiMo/MiMo-VL) ![](https://img.shields.io/github/stars/XiaomiMiMo/MiMo-VL?style=social) | [Hugging_Face](https://huggingface.co/collections/XiaomiMiMo/mimo-vl) | `Text` `Video` | 2025-06 | [![arXiv](https://img.shields.io/badge/arXiv-2506.03569-b31b1b.svg?style=plastic)](https://arxiv.org/abs/2506.03569) |
| [Video-Skill-CoT: Skill-based Chain-of-Thoughts for Domain-Adaptive Video Reasoning](https://arxiv.org/abs/2506.03525) | [GitHub](https://github.com/daeunni/Video-Skill-CoT) ![](https://img.shields.io/github/stars/daeunni/Video-Skill-CoT?style=social) | `N/A` | `Text` `Video` | 2025-06 |  [![](https://img.shields.io/badge/EMNLP-2025(Findings)-blue.svg?style=plastic)]() |
| [EgoVLM: Policy Optimization for Egocentric Video Understanding](https://arxiv.org/abs/2506.03097) | [GitHub](https://github.com/adityavavre/VidEgoVLM) ![](https://img.shields.io/github/stars/adityavavre/VidEgoVLM?style=social) |  [Hugging_Face](https://huggingface.co/datasets/omlab/VLM-R1) | `Text` `Video` | 2025-06 | [![arXiv](https://img.shields.io/badge/arXiv-2506.03097-b31b1b.svg?style=plastic)](https://arxiv.org/abs/2506.03097) |
| [Reinforcement Learning Tuning for VideoLLMs: Reward Design and Data Efficiency](https://arxiv.org/abs/2506.01908) | [GitHub](https://github.com/appletea233/Temporal-R1) ![](https://img.shields.io/github/stars/appletea233/Temporal-R1?style=social) | [Hugging_Face](https://huggingface.co/appletea2333/temporal-r1-7b-base) | `Text` `Video` | 2025-06 | [![arXiv](https://img.shields.io/badge/arXiv-2506.01908-b31b1b.svg?style=plastic)](https://arxiv.org/abs/2506.01908) |
| [VideoCap-R1: Enhancing MLLMs for Video Captioning via Structured Thinking](https://arxiv.org/abs/2506.01725) | `N/A` | `N/A` | `Text` `Video` | 2025-06 | [![arXiv](https://img.shields.io/badge/arXiv-2506.01725-b31b1b.svg?style=plastic)](https://arxiv.org/abs/2506.01725) |
| [ReAgent-V: A Reward-Driven Multi-Agent Framework for Video Understanding](https://arxiv.org/abs/2506.01300) | [GitHub](https://github.com/aiming-lab/ReAgent-V) ![](https://img.shields.io/github/stars/aiming-lab/ReAgent-V?style=social) | `N/A` | `Text` `Video` | 2025-06 |  [![](https://img.shields.io/badge/NeurIPS-2025-blue.svg?style=plastic)]() |
| [ReFoCUS: Reinforcement-guided Frame Optimization for Contextual Understanding](https://arxiv.org/abs/2506.01274) | `N/A` | `N/A` | `Text` `Video` | 2025-06 | [![arXiv](https://img.shields.io/badge/arXiv-2506.01274-b31b1b.svg?style=plastic)](https://arxiv.org/abs/2506.01274) |
| [DIVE: Deep-search Iterative Video Exploration](https://arxiv.org/abs/2506.21891) | [Github](https://github.com/PanasonicConnect/DIVE) ![](https://img.shields.io/github/stars/PanasonicConnect/DIVE?style=social) | `N/A` | `Text` `Video` | 2025-06 | [![](https://img.shields.io/badge/CVPR-2025(Challenge)-blue.svg?style=plastic)]() |
| [VideoDeepResearch: Long Video Understanding With Agentic Tool Using](https://arxiv.org/abs/2506.10821) | [Github](https://github.com/yhy-2000/VideoDeepResearch) ![](https://img.shields.io/github/stars/yhy-2000/VideoDeepResearch?style=social) | `N/A` | `Text` `Video` | 2025-06 | [![arXiv](https://img.shields.io/badge/arXiv-2506.10821-b31b1b.svg?style=plastic)](https://arxiv.org/abs/2506.10821) |
| [Wait, We Don't Need to "Wait"! Removing Thinking Tokens Improves Reasoning Efficiency](https://arxiv.org/abs/2506.08343) | `N/A` | `N/A` | `Text` `Video` | 2025-06 | [![arXiv](https://img.shields.io/badge/arXiv-2506.08343-b31b1b.svg?style=plastic)](https://arxiv.org/abs/2506.08343) |
| [DeepVideo-R1: Video Reinforcement Fine-Tuning via Difficulty-aware Regressive GRPO](https://arxiv.org/abs/2506.07464) | [Github](https://github.com/mlvlab/DeepVideoR1) ![](https://img.shields.io/github/stars/mlvlab/DeepVideoR1?style=social) | `N/A` | `Text` `Video` | 2025-06 |  [![](https://img.shields.io/badge/NeurIPS-2025-blue.svg?style=plastic)](https://neurips.cc/virtual/2025/loc/san-diego/poster/120134) |
| [Video-CoT: A Comprehensive Dataset for Spatiotemporal Understanding of Videos Based on Chain-of-Thought](https://dl.acm.org/doi/10.1145/3746027.3758313) | `N/A` | [Project_Page](https://video-CoT.github.io/) | `Text` `Video` | 2025-06 |  [![](https://img.shields.io/badge/ACM--MM-2025-blue.svg?style=plastic)](https://dl.acm.org/doi/10.1145/3746027.3758313)|
| [VideoChat-A1: Thinking with Long Videos by Chain-of-Shot Reasoning](https://arxiv.org/abs/2506.06097) | `N/A` | `N/A` | `Text` `Video` | 2025-06 | [![arXiv](https://img.shields.io/badge/arXiv-2506.06097-b31b1b.svg?style=plastic)](https://arxiv.org/abs/2506.06097) |
| [Chain-of-Frames: Advancing Video Understanding in Multimodal LLMs via Frame-Aware Reasoning](https://arxiv.org/abs/2506.00318) | [Github](https://github.com/SaraGhazanfari/CoF) ![](https://img.shields.io/github/stars/SaraGhazanfari/CoF?style=social) | [Hugging_Face](https://huggingface.co/saraghznfri/CoF-models) | `Text` `Video` | 2025-06 | [![arXiv](https://img.shields.io/badge/arXiv-2506.00318-b31b1b.svg?style=plastic)](https://arxiv.org/abs/2506.00318) |
| [Reinforcing Video Reasoning with Focused Thinking](https://arxiv.org/abs/2505.24718) |  [Github](https://github.com/longmalongma/TW-GRPO) ![](https://img.shields.io/github/stars/longmalongma/TW-GRPO?style=social)  | [Hugging_Face](https://huggingface.co/Falconss1/TW-GRPO)| `Text` `Video` | 2025-05 | [![arXiv](https://img.shields.io/badge/arXiv-2505.24718-b31b1b.svg?style=plastic)](https://arxiv.org/abs/2505.24718) |
| [A2Seek: Towards Reasoning-Centric Benchmark for Aerial Anomaly Understanding](https://arxiv.org/abs/2505.21962) |  [Github](https://github.com/2-mo/A2Seek) ![](https://img.shields.io/github/stars/2-mo/A2Seek?style=social)  | `N/A` | `Text` `Video` | 2025-05 | [![arXiv](https://img.shields.io/badge/arXiv-2505.21962-b31b1b.svg?style=plastic)](https://arxiv.org/abs/2505.21962) |
| [Omni-R1: Reinforcement Learning for Omnimodal Reasoning via Two-System Collaboration](https://arxiv.org/abs/2505.20256) |  [Github](https://github.com/aim-uofa/Omni-R1) ![](https://img.shields.io/github/stars/aim-uofa/Omni-R1?style=social)  | [Hugging_Face](https://huggingface.co/Haoz0206/Omni-R1) | `Text` `Audio` `Video` | 2025-05 | [![](https://img.shields.io/badge/NeurIPS-2025-blue.svg?style=plastic)](https://neurips.cc/virtual/2025/poster/119706) |
| [Vad-R1: Towards Video Anomaly Reasoning via Perception-to-Cognition Chain-of-Thought](https://arxiv.org/abs/2505.19877) |  [Github](https://github.com/wbfwonderful/Vad-R1) ![](https://img.shields.io/github/stars/wbfwonderful/Vad-R1?style=social)  | `N/A` | `Text` `Video` | 2025-05 | [![](https://img.shields.io/badge/NeurIPS-2025-blue.svg?style=plastic)](https://neurips.cc/virtual/2025/poster/119777) |
| [VerIPO: Cultivating Long Reasoning in Video-LLMs via Verifier-Guided Iterative Policy Optimization](https://arxiv.org/abs/2505.19000) |  [Github](https://github.com/HITsz-TMG/VerIPO) ![](https://img.shields.io/github/stars/HITsz-TMG/VerIPO?style=social) | [Hugging_Face](https://huggingface.co/Uni-MoE/VerIPO-7B-v1.0) | `Text` `Video` | 2025-05 | [![arXiv](https://img.shields.io/badge/arXiv-2505.19000-b31b1b.svg?style=plastic)](https://arxiv.org/abs/2505.19000) |
| [Fact-R1: Towards Explainable Video Misinformation Detection with Deep Reasoning](https://arxiv.org/abs/2505.16836) | [Github](https://github.com/zfr00/Fact-R1) ![](https://img.shields.io/github/stars/zfr00/Fact-R1?style=social) | `N/A`  | `Text` `Speech` `Video` | 2025-05 |  [![](https://img.shields.io/badge/NeurIPS-2025-blue.svg?style=plastic)](https://neurips.cc/virtual/2025/loc/san-diego/poster/119113) |
| [Pixel Reasoner: Incentivizing Pixel-Space Reasoning with Curiosity-Driven Reinforcement Learning](https://arxiv.org/abs/2505.15966) |  [Github](https://github.com/TIGER-AI-Lab/Pixel-Reasoner) ![](https://img.shields.io/github/stars/TIGER-AI-Lab/Pixel-Reasoner?style=social)  | [Hugging_Face](https://huggingface.co/TIGER-Lab/PixelReasoner-RL-v1) | `Text` `Video` | 2025-05 |  [![](https://img.shields.io/badge/NeurIPS-2025-blue.svg?style=plastic)](https://neurips.cc/virtual/2025/poster/117667) |
| [UniVG-R1: Reasoning Guided Universal Visual Grounding with Reinforcement Learning](https://arxiv.org/abs/2505.14231) |  [Github](https://github.com/AMAP-ML/UniVG-R1) ![](https://img.shields.io/github/stars/AMAP-ML/UniVG-R1?style=social)  | [Hugging_Face](https://huggingface.co/GD-ML/UniVG-R1) | `Text` `Video` | 2025-05 | [![arXiv](https://img.shields.io/badge/arXiv-2505.14231-b31b1b.svg?style=plastic)](https://arxiv.org/abs/2505.14231) |
| [VideoRFT: Incentivizing Video Reasoning Capability in MLLMs via Reinforced Fine-Tuning](https://arxiv.org/abs/2505.12434) |  [Github](https://github.com/QiWang98/VideoRFT) ![](https://img.shields.io/github/stars/QiWang98/VideoRFT?style=social) |  [Hugging_Face](https://huggingface.co/QiWang98/VideoRFT) | `Text` `Video` | 2025-05 | [![](https://img.shields.io/badge/NeurIPS-2025-blue.svg?style=plastic)](https://neurips.cc/virtual/2025/poster/119996) |
| [Seed1.5-VL Technical Report](https://arxiv.org/abs/2505.07062) | `N/A` | `N/A` | `Text` `Video` | 2025-05 | [![arXiv](https://img.shields.io/badge/arXiv-2505.07062-b31b1b.svg?style=plastic)](https://arxiv.org/abs/2505.07062) |
| [TEMPURA: Temporal Event Masked Prediction and Understanding for Reasoning in Action](https://arxiv.org/abs/2505.01583) |  [Github](https://github.com/Andy-Cheng/TEMPURA) ![](https://img.shields.io/github/stars/Andy-Cheng/TEMPURA?style=social)  | [Hugging_Face](https://huggingface.co/andaba/TEMPURA-Qwen2.5-VL-3B-s1) | `Text` `Video` | 2025-05 | [![arXiv](https://img.shields.io/badge/arXiv-2505.01583-b31b1b.svg?style=plastic)](https://arxiv.org/abs/2505.01583) |
| [Fostering Video Reasoning via Next-Event Prediction](https://arxiv.org/abs/2505.22457) |  [Github](https://github.com/sail-sg/Video-Next-Event-Prediction) ![](https://img.shields.io/github/stars/sail-sg/Video-Next-Event-Prediction?style=social)  | `N/A` | `Text` `Video` | 2025-05 | [![arXiv](https://img.shields.io/badge/arXiv-2505.22457-b31b1b.svg?style=plastic)](https://arxiv.org/abs/2505.22457) |
| [SiLVR: A Simple Language-based Video Reasoning Framework](https://arxiv.org/abs/2505.24869) |  [Github](https://github.com/CeeZh/SILVR) ![](https://img.shields.io/github/stars/CeeZh/SILVR?style=social)  | - | `Text` `Video` | 2025-05 | [![arXiv](https://img.shields.io/badge/arXiv-2505.24869-b31b1b.svg?style=plastic)](https://arxiv.org/abs/2505.24869) |
| [RVTBench: A Benchmark for Visual Reasoning Tasks](https://arxiv.org/abs/2505.11838) | [GitHub](https://github.com/yiqings/rvt) ![](https://img.shields.io/github/stars/yiqings/rvt?style=social) |  [Hugging_Face](https://huggingface.co/datasets/yiqingshen/rvtbench/tree/main/rvtbench) | `Text` `Video` | 2025-05 | [![arXiv](https://img.shields.io/badge/arXiv-2505.11838-b31b1b.svg?style=plastic)](https://arxiv.org/abs/2505.11838) |
| [CoT-Vid: Dynamic Chain-of-Thought Routing with Self-Verification for Training-Free Video Reasoning](https://arxiv.org/abs/2505.11830) | `N/A` | `N/A` | `Text` `Video` | 2025-05 | [![](https://img.shields.io/badge/arXiv-2505.11830-b31b1b.svg?style=plastic)](https://arxiv.org/abs/2505.11830) |
| [VCRBench: Exploring Long-form Causal Reasoning Capabilities of Large Video Language Models](https://arxiv.org/abs/2505.08455) | [GitHub](https://github.com/pritamqu/VCRBench) ![](https://img.shields.io/github/stars/pritamqu/VCRBench?style=social) | `N/A` | `Text` `Video` | 2025-05 | [![](https://img.shields.io/badge/arXiv-2505.08455-b31b1b.svg?style=plastic)](https://arxiv.org/abs/2505.08455) |
| [AVA: Towards Agentic Video Analytics with Vision Language Models](https://arxiv.org/abs/2505.00254) | [GitHub](https://github.com/I-ESC/Project-Ava) ![](https://img.shields.io/github/stars/I-ESC/Project-Ava?style=social) | `N/A` | `Text` `Video` | 2025-05 | [![](https://img.shields.io/badge/NSDI-2026-blue.svg?style=plastic)](https://www.usenix.org/conference/nsdi26/presentation/yan) |
| [TinyLLaVA-Video-R1: Towards Smaller LMMs for Video Reasoning](https://arxiv.org/abs/2504.09641) |  [GitHub](https://github.com/ZhangXJ199/TinyLLaVA-Video-R1) ![](https://img.shields.io/github/stars/ZhangXJ199/TinyLLaVA-Video-R1?style=social) | [Hugging_Face](https://huggingface.co/Zhang199/TinyLLaVA-Video-R1)| `Text` `Video` | 2025-04 | [![arXiv](https://img.shields.io/badge/arXiv-2504.09641-b31b1b.svg?style=plastic)](https://arxiv.org/abs/2504.09641) |
| [VideoChat-R1: Enhancing Spatio-Temporal Perception via Reinforcement Fine-Tuning](https://arxiv.org/abs/2504.06958) | [GitHub](https://github.com/OpenGVLab/VideoChat-R1) ![](https://img.shields.io/github/stars/OpenGVLab/VideoChat-R1?style=social)| [Hugging_Face](https://huggingface.co/collections/OpenGVLab/videochat-r1) | `Text` `Video` | 2025-04 | [![arXiv](https://img.shields.io/badge/arXiv-2504.06958-b31b1b.svg?style=plastic)](https://arxiv.org/abs/2504.06958) |
| [Spatial-R1: Enhancing MLLMs in Video Spatial Reasoning](https://arxiv.org/abs/2504.01805) | - | [Hugging_Face](https://huggingface.co/datasets/RUBBISHLIKE/SpaceR-151k) | `Text` `Video` | 2025-04 | [![arXiv](https://img.shields.io/badge/arXiv-2504.01805-b31b1b.svg?style=plastic)](https://arxiv.org/abs/2504.01805) |
| [Improved Visual-Spatial Reasoning via R1-Zero-Like Training](https://arxiv.org/abs/2504.00883) | - | - | `Text` `Video` | 2025-04 | [![arXiv](https://img.shields.io/badge/arXiv-2504.00883-b31b1b.svg?style=plastic)](https://arxiv.org/abs/2504.00883) |
| [Eagle 2.5: Boosting Long-Context Post-Training for Frontier Vision-Language Models](https://arxiv.org/abs/2504.15271) | - | - | `Text` `Video` | 2025-04 | [![arXiv](https://img.shields.io/badge/arXiv-2504.15271-b31b1b.svg?style=plastic)](https://arxiv.org/abs/2504.15271) |
| [LVC: A Lightweight Compression Framework for Enhancing VLMs in Long Video Understanding](https://arxiv.org/abs/2504.06835) | - | - | `Text` `Video` | 2025-04 | [![arXiv](https://img.shields.io/badge/arXiv-2504.06835-b31b1b.svg?style=plastic)](https://arxiv.org/abs/2504.06835) |
| [From 128K to 4M: Efficient Training of Ultra-Long Context Large Language Models](https://arxiv.org/abs/2504.06214) | - | - | `Text` `Video` | 2025-04 | [![arXiv](https://img.shields.io/badge/arXiv-2504.06214-b31b1b.svg?style=plastic)](https://arxiv.org/abs/2504.06214) |
| [MR. Video: "MapReduce" is the Principle for Long Video Understanding](https://arxiv.org/abs/2504.16082) | - | - | `Text` `Video` | 2025-04 | [![arXiv](https://img.shields.io/badge/arXiv-2504.16082-b31b1b.svg?style=plastic)](https://arxiv.org/abs/2504.16082) |
| [Multimodal Long Video Modeling Based on Temporal Dynamic Context](https://arxiv.org/abs/2504.10443) | - | - | `Text` `Video` | 2025-04 | [![arXiv](https://img.shields.io/badge/arXiv-2504.10443-b31b1b.svg?style=plastic)](https://arxiv.org/abs/2504.10443) |
| [WikiVideo: Article Generation from Multiple Videos](https://arxiv.org/abs/2504.00939) | - | - | `Text` `Video` | 2025-04 | [![arXiv](https://img.shields.io/badge/arXiv-2504.00939-b31b1b.svg?style=plastic)](https://arxiv.org/abs/2504.00939) |
| [VCR-Bench: A Comprehensive Evaluation Framework for Video Chain-of-Thought Reasoning](https://arxiv.org/abs/2504.07956) | - | [Hugging_Face](https://vlm-reasoning.github.io/VCR-Bench/) | `Text` `Video` | 2025-04 | [![](https://img.shields.io/badge/arXiv-2504.07956-b31b1b.svg?style=plastic)](https://arxiv.org/abs/2504.07956) |
| [Exploring the Effect of Reinforcement Learning on Video Understanding: Insights from SEED-Bench-R1](https://arxiv.org/abs/2503.24376) | - |  [Hugging_Face](https://huggingface.co/datasets/TencentARC/SEED-Bench-R1) | `Text` `Video` | 2025-03 | [![arXiv](https://img.shields.io/badge/arXiv-2503.24376-b31b1b.svg?style=plastic)](https://arxiv.org/abs/2503.24376) |
| [Video-R1: Reinforcing Video Reasoning in MLLMs](https://arxiv.org/abs/2503.21776) | - |  [Hugging_Face](https://huggingface.co/datasets/Video-R1/Video-R1-data) | `Text` `Video` | 2025-03 | [![arXiv](https://img.shields.io/badge/arXiv-2503.21776-b31b1b.svg?style=plastic)](https://arxiv.org/abs/2503.21776) |
| [TimeZero: Temporal Video Grounding with Reasoning-Guided LVLM](https://arxiv.org/abs/2503.13377) | - | [Hugging_Face](https://huggingface.co/datasets/Boshenxx/TimeR1-Dataset) | `Text` `Video` | 2025-03 | [![arXiv](https://img.shields.io/badge/arXiv-2503.13377-b31b1b.svg?style=plastic)](https://arxiv.org/abs/2503.13377) |
| [ST-Think: How Multimodal Large Language Models Reason About 4D Worlds from Ego-Centric Videos](https://arxiv.org/abs/2503.12542) | - | - | `Text` `Video` | 2025-03 | [![arXiv](https://img.shields.io/badge/arXiv-2503.12542-b31b1b.svg?style=plastic)](https://arxiv.org/abs/2503.12542) |
| [VideoMind: A Chain-of-LoRA Agent for Long Video Reasoning](https://arxiv.org/abs/2503.13444) | - |  [Hugging_Face](https://huggingface.co/datasets/yeliudev/VideoMind-Dataset/tree/main) | `Text` `Video` | 2025-03 | [![arXiv](https://img.shields.io/badge/arXiv-2503.13444-b31b1b.svg?style=plastic)](https://arxiv.org/abs/2503.13444) |
| [Aurelia: Test-time Reasoning Distillation in Audio-Visual LLMs](https://arxiv.org/abs/2503.23219) | - | - | `Audio` `Video` `Text` | 2025-03 | [![arXiv](https://img.shields.io/badge/arXiv-2503.23219-b31b1b.svg?style=plastic)](https://arxiv.org/abs/2503.23219) |
| [video-SALMONN-o1: Reasoning-enhanced Audio-visual Large Language Model](https://arxiv.org/abs/2502.11775) | - | - | `Audio` `Video` `Text` | 2025-02 | [![arXiv](https://img.shields.io/badge/arXiv-2502.11775-b31b1b.svg?style=plastic)](https://arxiv.org/abs/2502.11775) |
| [TheoremExplainAgent: Towards Multimodal Explanations for LLM Theorem Understanding](https://arxiv.org/abs/2502.19400) | - |  [Hugging_Face](https://huggingface.co/datasets/TIGER-Lab/TheoremExplainBench) | `Text` `Video` | 2025-02 |  [![](https://img.shields.io/badge/ACL-2025(Main)-blue.svg?style=plastic)]() |
| [CoS: Chain-of-Shot Prompting for Long Video Understanding](https://arxiv.org/abs/2502.06428) |[GitHub](https://github.com/lwpyh/CoS_codes) ![](https://img.shields.io/github/stars/lwpyh/CoS_codes?style=social) |  - | `Text` `Video` | 2025-02 | [![arXiv](https://img.shields.io/badge/arXiv-2502.06428-b31b1b.svg?style=plastic)](https://arxiv.org/abs/2502.06428) |
| [Temporal Preference Optimization for Long-Form Video Understanding](https://arxiv.org/abs/2501.13919) |  [GitHub](https://github.com/ruili33/TPO) ![](https://img.shields.io/github/stars/ruili33/TPO?style=social) |  [Hugging_Face](https://huggingface.co/collections/ruili0/temporal-preference-optimization) | `Text` `Video` | 2025-01 | [![arXiv](https://img.shields.io/badge/arXiv-2501.13919-b31b1b.svg?style=plastic)](https://arxiv.org/abs/2501.13919) |
| [InternLM-XComposer2.5-Reward: A Simple Yet Effective Multi-Modal Reward Model](https://arxiv.org/abs/2501.12368) | [GitHub](https://github.com/InternLM/InternLM-XComposer) ![](https://img.shields.io/github/stars/InternLM/InternLM-XComposer?style=social) |  [Hugging_Face](https://huggingface.co/internlm/internlm-xcomposer2d5-7b-reward) | `Text` `Video` | 2025-01 |  [![](https://img.shields.io/badge/ACL-2025(Findings)-blue.svg?style=plastic)]() |
| [MECD+: Unlocking Event-Level Causal Graph Discovery for Video Reasoning](https://arxiv.org/abs/2501.07227) | [GitHub](https://github.com/tychen-SJTU/MECD-Benchmark) ![](https://img.shields.io/github/stars/tychen-SJTU/MECD-Benchmark?style=social) |  [Hugging_Face](https://github.com/tychen-SJTU/MECD-Benchmark) | `Text` `Video` | 2025-01 |   ![](https://img.shields.io/badge/IEEE-TPAMI-blue.svg?style=plastic) |
| [Building a Mind Palace: Structuring Environment-Grounded Semantic Graphs for Effective Long Video Analysis with LLMs](https://arxiv.org/abs/2501.04336) | - | - | `Text` `Video` | 2025-01 | [![arXiv](https://img.shields.io/badge/arXiv-2501.04336-b31b1b.svg?style=plastic)](https://arxiv.org/abs/2501.04336) |
| [Video-of-Thought: Step-by-Step Video Reasoning from Perception to Cognition](https://arxiv.org/abs/2501.03230) | [GitHub](https://github.com/scofield7419/Video-of-Thought) ![](https://img.shields.io/github/stars/scofield7419/Video-of-Thought?style=social)  | - | `Text` `Video` | 2025-01 |  ![](https://img.shields.io/badge/ICML-2024(Oral)-blue.svg?style=plastic) |
| [Expanding Performance Boundaries of Open-Source Multimodal Models with Model, Data, and Test-Time Scaling](https://arxiv.org/abs/2412.05271) |  [GitHub](https://github.com/OpenGVLab/InternVL) ![](https://img.shields.io/github/stars/OpenGVLab/InternVL?style=social)  | [Hugging_Face](https://github.com/OpenGVLab/InternVL) | `Text` `Video` | 2024-12 | [![arXiv](https://img.shields.io/badge/arXiv-2412.05271-b31b1b.svg?style=plastic)](https://arxiv.org/abs/2412.05271) |
| [STEP: Enhancing Video-LLMs' Compositional Reasoning by Spatio-Temporal Graph-guided Self-Training](https://arxiv.org/abs/2412.00161) | - | - | `Text` `Video` | 2024-12 | [![](https://img.shields.io/badge/CVPR-2025-blue.svg?style=plastic)](https://openaccess.thecvf.com/content/CVPR2025/html/Qiu_STEP_Enhancing_Video-LLMs_Compositional_Reasoning_by_Spatio-Temporal_Graph-guided_Self-Training_CVPR_2025_paper.html) |
| [VideoEspresso: A Large-Scale Chain-of-Thought Dataset for Fine-Grained Video Reasoning via Core Frame Selection](https://openaccess.thecvf.com/content/CVPR2025/html/Han_VideoEspresso_A_Large-Scale_Chain-of-Thought_Dataset_for_Fine-Grained_Video_Reasoning_via_CVPR_2025_paper.html) | [GitHub](https://github.com/hshjerry/VideoEspresso) ![](https://img.shields.io/github/stars/hshjerry/VideoEspresso?style=social) |  [Hugging_Face](https://huggingface.co/datasets/hshjerry0315/VideoEspresso-Test) | `Text` `Video` | 2024-11 | [![](https://img.shields.io/badge/CVPR-2025-blue.svg?style=plastic)](https://openaccess.thecvf.com/content/CVPR2025/html/Han_VideoEspresso_A_Large-Scale_Chain-of-Thought_Dataset_for_Fine-Grained_Video_Reasoning_via_CVPR_2025_paper.html) |
| [Adaptive Video Understanding Agent: Enhancing efficiency with dynamic frame sampling and feedback-driven reasoning](https://arxiv.org/abs/2410.20252) | - | - | `Text` `Video` | 2024-10 | [![](https://img.shields.io/badge/NeurIPS-2024(Workshop)-blue.svg?style=plastic)]() |
| [VideoINSTA: Zero-shot Long Video Understanding via Informative Spatial-Temporal Reasoning with LLMs](https://arxiv.org/abs/2409.20365) | [GitHub](https://github.com/mayhugotong/VideoINSTA) ![](https://img.shields.io/github/stars/mayhugotong/VideoINSTA?style=social) | - | `Text` `Video` | 2024-09 | [![](https://img.shields.io/badge/EMNLP-2024(Findings)-blue.svg?style=plastic)]() |
| [MECD: Unlocking Multi-Event Causal Discovery in Video Reasoning](https://arxiv.org/abs/2409.17647) | [GitHub](https://github.com/tychen-SJTU/MECD-Benchmark) ![](https://img.shields.io/github/stars/tychen-SJTU/MECD-Benchmark?style=social) |  [Hugging_Face](https://github.com/tychen-SJTU/MECD-Benchmark) | `Text` `Video` | 2024-09 |  [![](https://img.shields.io/badge/NeurIPS-2024(Spotlight)-blue.svg?style=plastic)]() |

### üïπÔ∏è CoF-based Video Reasoning
| **Title** | **Code** | **Checkpoint** | **Time** | **Venue** |
|:-----------|:------------------:|:----------------:|:----------:|:-----------:|
| [Unified Video Editing with Temporal Reasoner](https://arxiv.org/abs/2512.07469) | [GitHub](https://github.com/knightyxp/VideoCoF) ![](https://img.shields.io/github/stars/knightyxp/VideoCoF?style=social)|[Hugging_Face](https://huggingface.co/XiangpengYang/VideoCoF)|2025-12| [![](https://img.shields.io/badge/arXiv-2512.07469-b31b1b.svg?style=plastic)](https://arxiv.org/abs/2512.07469) |
| [Video Models Start to Solve Chess, Maze, Sudoku, Mental Rotation, and Raven‚Äô Matrices](https://arxiv.org/abs/2512.05969) | [GitHub](https://github.com/Video-Reason/VMEvalKit) ![](https://img.shields.io/github/stars/Video-Reason/VMEvalKit?style=social)||2025-12| [![](https://img.shields.io/badge/arXiv-2512.05969-b31b1b.svg?style=plastic)](https://arxiv.org/abs/2512.05969) |
| [McSc: Motion-Corrective Preference Alignment for Video Generation with Self-Critic Hierarchical Reasoning](https://arxiv.org/abs/2511.22974) | [GitHub](https://github.com/QiushiYang/McSc) ![](https://img.shields.io/github/stars/QiushiYang/McSc?style=social)|-|2025-11| [![](https://img.shields.io/badge/arXiv-2511.22974-b31b1b.svg?style=plastic)](https://arxiv.org/abs/2511.22974) |
| [In-Video Instructions: Visual Signals as Generative Control](https://arxiv.org/abs/2511.19401) | [GitHub](https://github.com/VainF/In-Video-Instructions) ![](https://img.shields.io/github/stars/VainF/In-Video-Instructions?style=social)|-|2025-11| [![](https://img.shields.io/badge/arXiv-2511.19401-b31b1b.svg?style=plastic)](https://arxiv.org/abs/2511.19401) |
| [Video-as-Answer: Predict and Generate Next Video Event with Joint-GRPO](https://arxiv.org/abs/2511.16669) | [GitHub](https://github.com/KlingTeam/VANS) ![](https://img.shields.io/github/stars/KlingTeam/VANS?style=social)|[Hugging_Face](https://huggingface.co/KlingTeam/VANS)|2025-11| [![](https://img.shields.io/badge/arXiv-2511.16669-b31b1b.svg?style=plastic)](https://arxiv.org/abs/2511.16669) |
| [Reasoning via Video: The First Evaluation of Video Models‚Äô Reasoning Abilities through Maze-Solving Tasks](https://arxiv.org/abs/2511.15065) | [GitHub](https://github.com/ImYangC7/VR-Bench) ![](https://img.shields.io/github/stars/ImYangC7/VR-Bench?style=social) | [Hugging_Face](https://huggingface.co/HY-Wan/Wan-R1) | 2025-11 |[![arXiv](https://img.shields.io/badge/arXiv-2511.15065-b31b1b.svg?style=plastic)](https://arxiv.org/abs/2511.15065)  |
| [Thinking with Video: Video Generation as a Promising Multimodal Reasoning Paradigm](https://arxiv.org/abs/2511.04570)| [GitHub](https://github.com/tongjingqi/Thinking-with-Video) ![](https://img.shields.io/github/stars/tongjingqi/Thinking-with-Video) | - | 2025-11| ![](https://img.shields.io/badge/arXiv-2511.04570-b31b1b.svg?style=plastic)|
| [Are Video Models Ready as Zero-Shot Reasoners? An Empirical Study with the MME-CoF Benchmark](https://arxiv.org/abs/2510.26802) | [GitHub](https://github.com/ZiyuGuo99/MME-CoF) ![](https://img.shields.io/github/stars/ZiyuGuo99/MME-CoF?style=social) |  [Hugging_Face](https://huggingface.co/datasets/ZiyuG/MME-CoF) | 2025-10 | [![](https://img.shields.io/badge/arXiv-2510.26802-b31b1b.svg?style=plastic)](https://arxiv.org/abs/2510.26802) | 
| [VChain : Chain-of-Visual-Thought for Reasoning in Video Generation](https://arxiv.org/abs/2510.05094)|  [GitHub](https://github.com/Eyeline-Labs/VChain) ![](https://img.shields.io/github/stars/Eyeline-Labs/VChain?style=social) | - |2025-10|![](https://img.shields.io/badge/arXiv-2510.26802-b31b1b.svg?style=plastic) |
| [Chain-of-Frames: Advancing Video Understanding in Multimodal LLMs via Frame-Aware Reasoning](https://arxiv.org/abs/2506.00318) | [GitHub](https://github.com/SaraGhazanfari/CoF) ![](https://img.shields.io/github/stars/SaraGhazanfari/CoF?style=social) | - | 2025-06 | ![](https://img.shields.io/badge/arXiv-2506.00318-b31b1b.svg?style=plastic) |



### üåà Interleaved Video Reasoning

| **Title** | **Code** |  **Checkpoint** | **Time** | **Venue** |
|:----------|:----------:|--------------------|-----------|--|
| [LongVT: Incentivizing "Thinking with Long Videos" via Native Tool Calling](https://arxiv.org/abs/2511.20785)| [GitHub](https://github.com/EvolvingLMMs-Lab/LongVT) ![](https://img.shields.io/github/stars/EvolvingLMMs-Lab/LongVT?style=social)| [Hugging_Face](https://huggingface.co/collections/lmms-lab/longvt) |2025-11|[![arXiv](https://img.shields.io/badge/arXiv-2511.20785-b31b1b.svg?style=plastic)](https://arxiv.org/abs/2511.20785)|
| [JavisGPT: A Unified Multi-modal LLM for Sounding-Video Comprehension and Generation](https://javisverse.github.io/JavisGPT-page/) |[GitHub](https://github.com/JavisVerse/JavisGPT) ![](https://img.shields.io/github/stars/JavisVerse/JavisGPT?style=social)| -|2025-11| [![](https://img.shields.io/badge/NeurIPS-2025(Spotlight)-blue.svg?style=plastic)](https://neurips.cc/virtual/2025/loc/san-diego/poster/118457)|
| [Video-R4: Reinforcing Text-Rich Video Reasoning with Visual Rumination](https://arxiv.org/abs/2511.17490)| [GitHub](https://github.com/yunlong10/Video-R4) ![](https://img.shields.io/github/stars/yunlong10/Video-R4?style=social)| - | 2025-11 |[![arXiv](https://img.shields.io/badge/arXiv-2511.17490-b31b1b.svg?style=plastic)](https://arxiv.org/abs/2511.17490)|
| [Orion: A Unified Visual Agent for Multimodal Perception, Advanced Visual Reasoning and Execution](https://arxiv.org/abs/2511.14210) | - | - | 2025-11 | [![arXiv](https://img.shields.io/badge/arXiv-2511.14120-b31b1b.svg?style=plastic)](https://arxiv.org/abs/2511.14120)  |
| [Video-in-the-Loop: Span-Grounded Long Video QA with Interleaved Reasoning](https://arxiv.org/abs/2510.04022) |- | - | 2025-10 | [![arXiv](https://img.shields.io/badge/arXiv-2510.04022-b31b1b.svg?style=plastic)](https://arxiv.org/abs/2510.04022) |
| [ViTCoT: Video-Text Interleaved Chain-of-Thought for Boosting Video Understanding in Large Language Models](https://dl.acm.org/doi/abs/10.1145/3746027.3755837) |  [GitHub](https://github.com/BRZ911/ViTCoT) ![](https://img.shields.io/github/stars/BRZ911/ViTCoT?style=social)  | [Hugging_Face](https://huggingface.co/datasets/BRZ911/ViTCoT)| 2025-10 | [![](https://img.shields.io/badge/ACM--MM-2025-blue.svg?style=plastic)](https://dl.acm.org/doi/10.1145/3746027.3758313)|
| [FrameMind: Frame-Interleaved Video Reasoning via Reinforcement Learning](https://arxiv.org/abs/2509.24008) | - | -   |2025-09 | [![arXiv](https://img.shields.io/badge/arXiv-2509.24008-b31b1b.svg?style=plastic)](https://arxiv.org/abs/2509.24008)  |
| [Thinking With Videos: Multimodal Tool-Augmented Reinforcement Learning for Long Video Reasoning](https://arxiv.org/abs/2508.04416) |  [GitHub](https://github.com/zhang9302002/ThinkingWithVideos) ![](https://img.shields.io/github/stars/zhang9302002/ThinkingWithVideos?style=social) | [Hugging_Face](https://huggingface.co/datasets/zhang9302002/MultiTaskVideoReasoning) | 2025-08 | [![arXiv](https://img.shields.io/badge/ACM-2508.04416-b31b1b.svg?style=plastic)](https://arxiv.org/abs/2508.04416) |
| [VILA-U: a Unified Foundation Model Integrating Visual Understanding and Generation](https://arxiv.org/abs/2409.04429) | [GitHub](https://github.com/mit-han-lab/vila-u) ![](https://img.shields.io/github/stars/mit-han-lab/vila-u?style=social) | [Hugging_Face](https://huggingface.co/collections/mit-han-lab/vila-u-7b)   |2024-09 | [![](https://img.shields.io/badge/ICLR-2025-blue.svg?style=plastic)]()|


### üîÅ Streaming Video Reasoning

| **Title** | **Code** |  **Checkpoint** | **Time** | **Venue** |
|:----------|:----------:|:----------:|:----------:|:-----------:|
| [LiveStar: Live Streaming Assistant for Real-World Online Video Understanding](https://arxiv.org/abs/2511.05299) | [GitHub](https://github.com/sotayang/LiveStar) ![](https://img.shields.io/github/stars/sotayang/LiveStar?style=social) | [Hugging_Face](https://huggingface.co/yzy666/LiveStar_8B) | 2025-11 |  [![](https://img.shields.io/badge/NeurIPS-2025-blue.svg?style=plastic)]() |
| [StreamingVLM: Real-Time Understanding for Infinite Video Streams](https://arxiv.org/abs/2510.09608) | [GitHub](https://github.com/mit-han-lab/streaming-vlm) ![](https://img.shields.io/github/stars/mit-han-lab/streaming-vlm?style=social) | / | 2025-10 | [![arXiv](https://img.shields.io/badge/arXiv-2510.09608-b31b1b.svg?style=plastic)](https://arxiv.org/abs/2510.09608) |
| [StreamAgent: Towards Anticipatory Agents for Streaming Video Understanding](https://arxiv.org/abs/2508.01875) | - | - | 2025-10 | [![arXiv](https://img.shields.io/badge/arXiv-2508.01875-b31b1b.svg?style=plastic)](https://arxiv.org/abs/2508.01875) |
| [StreamingCoT: A Dataset for Temporal Dynamics and Multimodal Chain-of-Thought Reasoning in Streaming VideoQA](https://arxiv.org/pdf/2510.25332) |[GitHub](https://github.com/Fleeting-hyh/StreamingCoT?tab=readme-ov-file#minerva) ![](https://img.shields.io/github/stars/Fleeting-hyh/StreamingCoT?style=social) | / | 2025-10 |[![](https://img.shields.io/badge/ACM--MM-2025-blue.svg?style=plastic)]()|
| [StreamForest: Efficient Online Video Understanding with Persistent Event Memory](https://arxiv.org/abs/2509.24871) |  [GitHub](https://github.com/MCG-NJU/StreamForest) ![](https://img.shields.io/github/stars/MCG-NJU/StreamForest?style=social)  | [Hugging_Face](https://huggingface.co/collections/MCG-NJU/streamforest-and-odvbench) | 2025-09 | [![](https://img.shields.io/badge/NeurIPS-2025(Spotlight)-blue.svg?style=plastic)]() |
| [StreamVLN: Streaming Vision-and-Language Navigation via SlowFast Context Modeling](https://arxiv.org/abs/2507.05240) | [GitHub](https://github.com/InternRobotics/StreamVLN) ![](https://img.shields.io/github/stars/InternRobotics/StreamVLN?style=social) | [Hugging_Face](https://huggingface.co/mengwei0427/StreamVLN_Video_qwen_1_5_r2r_rxr_envdrop_scalevln_v1_3) |  2025-07 | [![arXiv](https://img.shields.io/badge/arXiv-2507.05240-b31b1b.svg?style=plastic)](https://arxiv.org/abs/2507.05240) |
| [Flash-VStream: Efficient Real-Time Understanding for Long Video Streams](https://arxiv.org/abs/2506.23825) | [GitHub](https://github.com/IVGSZ/Flash-VStream) ![](https://img.shields.io/github/stars/IVGSZ/Flash-VStream?style=social) | [Hugging_Face](https://huggingface.co/zhang9302002/Flash-VStream-Qwen-7b) | 2025-06 | [![](https://img.shields.io/badge/ICCV-2025-blue.svg?style=plastic)]()  |
| [StreamBridge: Turning Your Offline Video Large Language Model into a Proactive Streaming Assistant](https://arxiv.org/abs/2505.05467) | [GitHub](https://github.com/apple/ml-streambridge) ![](https://img.shields.io/github/stars/apple/ml-streambridge?style=social) | / | 2025-05 |[![](https://img.shields.io/badge/NeurIPS-2025-blue.svg?style=plastic)]()  |
| [LiveVLM: Efficient Online Video Understanding via Streaming-Oriented KV Cache and Retrieval](https://arxiv.org/abs/2505.15269) | - | - | 2025-05 | [![arXiv](https://img.shields.io/badge/arXiv-2505.15269-b31b1b.svg?style=plastic)](https://arxiv.org/abs/2505.15269) |
| [TimeChat-Online: 80% Visual Tokens are Naturally Redundant in Streaming Videos](https://arxiv.org/abs/2504.17343) | [GitHub](https://github.com/yaolinli/TimeChat-Online) ![](https://img.shields.io/github/stars/yaolinli/TimeChat-Online?style=social) | [Hugging_Face](https://huggingface.co/wyccccc/TimeChatOnline-7B) | 2025-04 |  [![](https://img.shields.io/badge/ACM--MM-2025-blue.svg?style=plastic)]() |
| [LiveCC: Learning Video LLM with Streaming Speech Transcription at Scale](https://arxiv.org/abs/2504.16030) | [GitHub](https://github.com/showlab/livecc) ![](https://img.shields.io/github/stars/showlab/livecc?style=social) | - | 2025-04 | [![arXiv](https://img.shields.io/badge/arXiv-2504.16030-b31b1b.svg?style=plastic)](https://arxiv.org/abs/2504.16030) |
| [ViSpeak: Visual Instruction Feedback in Streaming Videos](https://arxiv.org/abs/2503.12769) | [GitHub](https://github.com/HumanMLLM/ViSpeak) ![](https://img.shields.io/github/stars/HumanMLLM/ViSpeak?style=social) | [Model_Zoo](https://github.com/HumanMLLM/ViSpeak#-model-zoo) | 2025-03 | [![](https://img.shields.io/badge/ICCV-2025-blue.svg?style=plastic)]()  |
| [StreamMind: Unlocking Full Frame Rate Streaming Video Dialogue through Event-Gated Cognition](https://arxiv.org/abs/2503.06220) | [GitHub](https://github.com/xinding-sys/StreamMind) ![](https://img.shields.io/github/stars/xinding-sys/StreamMind?style=social) | / | 2025-03 |[![](https://img.shields.io/badge/ICCV-2025-blue.svg?style=plastic)]() |
| [Streaming Video Question-Answering with In-context Video KV-Cache Retrieval](https://arxiv.org/abs/2503.00540) | [GitHub](https://github.com/Becomebright/ReKV) ![](https://img.shields.io/github/stars/Becomebright/ReKV?style=social) | / | 2025-03 | [![](https://img.shields.io/badge/ICLR-2025-blue.svg?style=plastic)]() |
| [SVBench: A Benchmark with Temporal Multi-Turn Dialogues for Streaming Video Understanding](https://arxiv.org/abs/2502.10810) | [GitHub](https://github.com/sotayang/SVBench) ![](https://img.shields.io/github/stars/sotayang/SVBench?style=social) | [Hugging_Face](https://huggingface.co/yzy666/StreamingChat_8B) | 2025-02 | [![](https://img.shields.io/badge/ICLR-2025(Spotlight)-blue.svg?style=plastic)]() |
| [Dispider: Enabling Video LLMs with Active Real-Time Interaction via Disentangled Perception, Decision, and Reaction](https://arxiv.org/abs/2501.03218) | [GitHub](https://github.com/Mark12Ding/Dispider) ![](https://img.shields.io/github/stars/Mark12Ding/Dispider?style=social) | [Hugging_Face](https://huggingface.co/Mar2Ding/Dispider) | 2025-01 | [![](https://img.shields.io/badge/CVPR-2025-blue.svg?style=plastic)]() |
| [Streaming Video Understanding and Multi-round Interaction with Memory-enhanced Knowledge](https://arxiv.org/abs/2501.13468) |  [GitHub](https://github.com/hmxiong/StreamChat) ![](https://img.shields.io/github/stars/hmxiong/StreamChat?style=social)  | / |  2025-01 | [![](https://img.shields.io/badge/ICLR-2025-blue.svg?style=plastic)]() |
| [Online Video Understanding: A Comprehensive Benchmark and Memory-Augmented Method](https://arxiv.org/abs/2501.00584) |  [GitHub](https://github.com/MCG-NJU/VideoChat-Online) ![](https://img.shields.io/github/stars/MCG-NJU/VideoChat-Online?style=social) | [Hugging_Face](https://huggingface.co/datasets/MCG-NJU/OVBench) | 2025-01 | [![](https://img.shields.io/badge/CVPR-2025-blue.svg?style=plastic)]() |
| [StreamChat: Chatting with Streaming Video](https://arxiv.org/abs/2412.08646) | / | / | 2024-11 | [![arXiv](https://img.shields.io/badge/arXiv-2412.08646-b31b1b.svg?style=plastic)](https://arxiv.org/abs/2412.08646) |




## ‚ú®Ô∏è Benchmarks

| **Name**      | **Paper** | **Link** | **Task** | **Time** | **Venue** |
|:--------------------:|:-----------|:-------------:|:----------:|:----------:|:-----------:|
| MMGR |[MMGR: Multi-Modal Generative Reasoning](https://arxiv.org/pdf/2512.14691)| [GitHub](https://github.com/Zefan-Cai/MMGR) ![](https://img.shields.io/github/stars/Zefan-Cai/MMGR?style=social)<br>[Hugging_Face](https://huggingface.co/datasets/ZefanCai/Video-Reasoning-Clean)  |`Vision`|2015-12| [![](https://img.shields.io/badge/arXiv-2512.14691-b31b1b.svg?style=plastic)](https://arxiv.org/abs/2512.14691)|
| MM-CoT |[MM-CoT:A Benchmark for Probing Visual Chain-of-Thought Reasoning in Multimodal Models](https://arxiv.org/pdf/2512.08228)| |`Language`|2015-12| [![](https://img.shields.io/badge/arXiv-2512.08228-b31b1b.svg?style=plastic)](https://arxiv.org/abs/2512.01989)|
| RULER-Bench |[RULER-Bench: Probing Rule-based Reasoning Abilities of Next-level Video Generation Models for Vision Foundation Intelligence](https://arxiv.org/abs/2512.02622) |  [GitHub](https://github.com/hexmSeeU/RULER-Bench) ![](https://img.shields.io/github/stars/hexmSeeU/RULER-Bench?style=social)<br>[Hugging_Face](https://huggingface.co/datasets/hexmSeeU/RULER-Bench) |`Vision` |2025-12| [![](https://img.shields.io/badge/arXiv-2512.02622-b31b1b.svg?style=plastic)](https://arxiv.org/abs/2512.02622) |
| AV-SpeakerBench |[See, Hear, and Understand: Benchmarking Audiovisual Human Speech Understanding in Multimodal Large Language Models](https://arxiv.org/abs/2512.02231) |  [GitHub](https://github.com/plnguyen2908/AV-SpeakerBench) ![](https://img.shields.io/github/stars/plnguyen2908/AV-SpeakerBench?style=social) |`Language` |2025-12| [![](https://img.shields.io/badge/arXiv-2512.01989-b31b1b.svg?style=plastic)](https://arxiv.org/abs/2512.01989) |
| PAI-Bench | [PAI-Bench: A Comprehensive Benchmark For Physical AI](https://arxiv.org/abs/2512.01989) |  [GitHub](https://github.com/SHI-Labs/physical-ai-bench) ![](https://img.shields.io/github/stars/SHI-Labs/physical-ai-bench?style=social) |`Language` `Vision` |2025-12| [![](https://img.shields.io/badge/arXiv-2512.01989-b31b1b.svg?style=plastic)](https://arxiv.org/abs/2512.01989) |
| Envision | [Envision: Benchmarking Unified Understanding & Generation for Causal World Process Insights](https://arxiv.org/abs/2512.01816) |  [GitHub](https://github.com/opendatalab-raiser/Envision) ![](https://img.shields.io/github/stars/opendatalab-raiser/Envision?style=social) |`Vision` |2025-12| [![](https://img.shields.io/badge/arXiv-2512.01816-b31b1b.svg?style=plastic)](https://arxiv.org/abs/2512.01816) |
| STREAMGAZE | [STREAMGAZE: Gaze-Guided Temporal Reasoning and Proactive Understanding in Streaming Videos](https://arxiv.org/abs/2512.01707) |  [GitHub](https://github.com/daeunni/StreamGaze) ![](https://img.shields.io/github/stars/daeunni/StreamGaze?style=social)<br>[Hugging_Face](https://huggingface.co/datasets/danaleee/StreamGaze) |`Streaming` `Language` |2025-12| [![](https://img.shields.io/badge/arXiv-2512.01816-b31b1b.svg?style=plastic)](https://arxiv.org/abs/2512.01707) |
| V-ReasonBench | [V-ReasonBench: Toward Unified Reasoning Benchmark Suite for Video Generation Models](https://arxiv.org/abs/2511.16668) | [GitHub](https://github.com/yangluo7/V-ReasonBench) ![](https://img.shields.io/github/stars/yangluo7/V-ReasonBench?style=social) | `Vision` | 2025-11 | [![](https://img.shields.io/badge/arXiv-2511.16668-b31b1b.svg?style=plastic)](https://arxiv.org/abs/2511.16668) |
| VR-Bench|[Reasoning via Video: The First Evaluation of Video Models‚Äô Reasoning Abilities through Maze-Solving Tasks](https://arxiv.org/abs/2511.15065) | [GitHub](https://github.com/ImYangC7/VR-Bench) ![](https://img.shields.io/github/stars/ImYangC7/VR-Bench?style=social)<br>[Hugging_Face](https://huggingface.co/datasets/amagipeng/VR-Bench) | `Vision` |2025-11 |[![arXiv](https://img.shields.io/badge/arXiv-2511.15065-b31b1b.svg?style=plastic)](https://arxiv.org/abs/2511.15065)  |
| Gen-ViRe   | [Can World Simulators Reason? Gen-ViRe: A Generative Visual Reasoning Benchmark](https://arxiv.org/abs/2511.13853) | [GitHub](https://github.com/L-CodingSpace/GVR) ![](https://img.shields.io/github/stars/L-CodingSpace/GVR?style=social)| `Vision` | 2025-11 |[![](https://img.shields.io/badge/arXiv-2511.13853-b31b1b.svg?style=plastic)](https://arxiv.org/abs/2511.13853)|
| TiViBench | [TiViBench: Benchmarking Think-in-Video Reasoning for Video Generative Models](https://arxiv.org/abs/2511.13704) | [GitHub](https://github.com/EnVision-Research/TiViBench) ![](https://img.shields.io/github/stars/EnVision-Research/TiViBench?style=social) | `Vision` | 2025-11 | [![](https://img.shields.io/badge/arXiv-2511.13704-b31b1b.svg?style=plastic)](https://arxiv.org/abs/2511.13704) |
| VideoThinkBench |[Thinking with Video: Video Generation as a Promising Multimodal Reasoning Paradigm](https://arxiv.org/abs/2511.04570)| [GitHub](https://github.com/tongjingqi/Thinking-with-Video) ![](https://img.shields.io/github/stars/tongjingqi/Thinking-with-Video) | `Vision` | 2025-11| ![](https://img.shields.io/badge/arXiv-2511.04570-b31b1b.svg?style=plastic)|
| MME-CoF | [Are Video Models Ready as Zero-Shot Reasoners? An Empirical Study with the MME-CoF Benchmark](https://arxiv.org/abs/2510.26802) | [Hugging_Face](https://huggingface.co/datasets/ZiyuG/MME-CoF) | `Vision` | 2025-10 | [![](https://img.shields.io/badge/arXiv-2510.26802-b31b1b.svg?style=plastic)](https://arxiv.org/abs/2510.26802) |
| SciVideoBench | [SciVideoBench: Benchmarking Scientific Video Reasoning in Large Multimodal Models](https://arxiv.org/abs/2510.08559) | [GitHub](https://github.com/dengandong/SciVideoBench) ![](https://img.shields.io/github/stars/dengandong/SciVideoBench?style=social) | `Language` | 2025-10 | [![](https://img.shields.io/badge/arXiv-2510.08559-b31b1b.svg?style=plastic)](https://arxiv.org/abs/2510.08559)  |
| ReasoningTrack | [ReasoningTrack: Chain-of-Thought Reasoning for Long-term Vision-Language Tracking](https://arxiv.org/abs/2508.05221) | [GitHub](https://github.com/Event-AHU/Open_VLTrack)![](https://img.shields.io/github/stars/Event-AHU/Open_VLTrack?style=social) | `Language` | 2025-08 | [![](https://img.shields.io/badge/arXiv-2508.05221-b31b1b.svg?style=plastic)](https://arxiv.org/abs/2508.05221) |
| METER | [METER: Multi-modal Evidence-based Thinking and Explainable Reasoning -- Algorithm and Benchmark](https://arxiv.org/abs/2507.16206) | - | `Language` | 2025-07 | [![](https://img.shields.io/badge/arXiv-2507.16206-b31b1b.svg?style=plastic)](https://arxiv.org/abs/2507.16206) |
| Video-TT | [Towards Video Thinking Test: A Holistic Benchmark for Advanced Video Reasoning and Understanding](https://arxiv.org/abs/2507.15028) | [Hugging_Face](https://huggingface.co/datasets/lmms-lab/video-tt) | `Language` | 2025-07 |  [![](https://img.shields.io/badge/ICCV-2025-blue.svg?style=plastic)]() |
| ImplicitQA | [ImplicitQA: Going beyond frames towards Implicit Video Reasoning](https://arxiv.org/abs/2506.21742) | [Hugging_Face](https://huggingface.co/datasets/ucf-crcv/ImplicitQA) | `Language` | 2025-06 | [![](https://img.shields.io/badge/arXiv-2506.21742-b31b1b.svg?style=plastic)](https://arxiv.org/abs/2506.21742) |
| Video-CoT | [Video-CoT: A Comprehensive Dataset for Spatiotemporal Understanding of Videos Based on Chain-of-Thought](https://arxiv.org/abs/2506.08817) | [Hugging_Face](https://huggingface.co/datasets/Zooy138/Video-CoT) | `Language` | 2025-06 | [![](https://img.shields.io/badge/arXiv-2506.08817-b31b1b.svg?style=plastic)](https://arxiv.org/abs/2506.08817) |
| Implicit-VideoQA | [Looking Beyond Visible Cues: Implicit Video Question Answering via Dual-Clue Reasoning](https://arxiv.org/abs/2506.07811) | [GitHub](https://github.com/tychen-SJTU/Implicit-VideoQA)![](https://img.shields.io/github/stars/tychen-SJTU/Implicit-VideoQA?style=social) | `Language` | 2025-06 | [![](https://img.shields.io/badge/arXiv-2506.07811-b31b1b.svg?style=plastic)](https://arxiv.org/abs/2506.07811) |
| MORSE-500 | [MORSE-500: A Programmatically Controllable Video Benchmark to Stress-Test Multimodal Reasoning](https://arxiv.org/abs/2506.05523) | [GitHub](https://github.com/morse-benchmark/morse-500) ![](https://img.shields.io/github/stars/morse-benchmark/morse-500?style=social)<br>[Hugging_Face](https://huggingface.co/datasets/video-reasoning/morse-500) | `Language` | 2025-06 | [![](https://img.shields.io/badge/arXiv-2506.05523-b31b1b.svg?style=plastic)](https://arxiv.org/abs/2506.05523) |
| SpookyBench | [Time Blindness: Why Video-Language Models Can't See What Humans Can](https://arxiv.org/abs/2505.24867) | [GitHub](https://github.com/TimeBlindness/time-blindness) ![](https://img.shields.io/github/stars/TimeBlindness/time-blindness?style=social)<br>[Hugging_Face](https://huggingface.co/datasets/timeblindness/spooky-bench) | `Language` | 2025-05 | [![](https://img.shields.io/badge/arXiv-2505.24867-b31b1b.svg?style=plastic)](https://arxiv.org/abs/2505.24867) |
| VideoReasonBench|[VideoReasonBench: Can MLLMs Perform Vision-Centric Complex Video Reasoning?](https://arxiv.org/abs/2505.23359) |  [GitHub](https://github.com/llyx97/video_reason_bench) ![](https://img.shields.io/github/stars/llyx97/video_reason_bench?style=social)<br>[Hugging_Face](https://huggingface.co/datasets/lyx97/reasoning_videos) | `Language` | 2025-05 | [![arXiv](https://img.shields.io/badge/arXiv-2505.23359-b31b1b.svg?style=plastic)](https://arxiv.org/abs/2505.23359) |
| Video-Holmes | [Video-Holmes: Can MLLM Think Like Holmes for Complex Video Reasoning?](https://arxiv.org/abs/2505.21374) | [GitHub](https://github.com/TencentARC/Video-Holmes) ![](https://img.shields.io/github/stars/TencentARC/Video-Holmes?style=social) | `Language` | 2025-05 | [![](https://img.shields.io/badge/arXiv-2505.21374-b31b1b.svg?style=plastic)](https://arxiv.org/abs/2505.21374) |
| VideoEval-Pro | [VideoEval-Pro: Robust and Realistic Long Video Understanding Evaluation](https://arxiv.org/abs/2505.14640) | [GitHub](https://github.com/TIGER-AI-Lab/VideoEval-Pro) ![](https://img.shields.io/github/stars/TIGER-AI-Lab/VideoEval-Pro?style=social)<br>[Hugging_Face](https://huggingface.co/datasets/TIGER-Lab/VideoEval-Pro) | `Language` | 2025-05 | [![](https://img.shields.io/badge/arXiv-2505.14640-b31b1b.svg?style=plastic)](https://arxiv.org/abs/2505.14640) |
| VBenchComp | [Breaking Down Video LLM Benchmarks](https://arxiv.org/abs/2505.14321) | - | `Language` | 2025-05 | [![](https://img.shields.io/badge/arXiv-2505.14321-b31b1b.svg?style=plastic)](https://arxiv.org/abs/2505.14321) |
| RVTBench|[RVTBench: A Benchmark for Visual Reasoning Tasks](https://arxiv.org/abs/2505.11838) |  [GitHub](https://github.com/yiqings/rvt) ![](https://img.shields.io/github/stars/yiqings/rvt?style=social)<br>[Hugging_Face](https://huggingface.co/datasets/yiqingshen/rvtbench/tree/main/rvtbench) |`Language` | 2025-05 | [![arXiv](https://img.shields.io/badge/arXiv-2505.11838-b31b1b.svg?style=plastic)](https://arxiv.org/abs/2505.11838) |
| VCRBench |[VCRBench: Exploring Long-form Causal Reasoning Capabilities of Large Video Language Models](https://arxiv.org/abs/2505.08455) | [GitHub](https://github.com/pritamqu/VCRBench) ![](https://img.shields.io/github/stars/pritamqu/VCRBench?style=social) | `Language` | 2025-05 | [![](https://img.shields.io/badge/arXiv-2505.08455-b31b1b.svg?style=plastic)](https://arxiv.org/abs/2505.08455) |
| RTV-Bench | [RTV-Bench: Benchmarking MLLM Continuous Perception, Understanding and Reasoning through Real-Time Video](https://arxiv.org/abs/2505.02064) | [GitHub](https://github.com/LJungang/RTV-Bench) ![](https://img.shields.io/github/stars/LJungang/RTV-Bench?style=social)<br>[Hugging_Face](https://huggingface.co/datasets/RTVBench/RTV-Bench) |`Streaming` `Language` | 2025-05 | [![](https://img.shields.io/badge/NeurIPS-2025(DB)-blue.svg?style=plastic)]() |
| MINERVA | [MINERVA: Evaluating Complex Video Reasoning](https://arxiv.org/abs/2505.00681) | [GitHub](https://github.com/google-deepmind/neptune?tab=readme-ov-file#minerva) ![](https://img.shields.io/github/stars/google-deepmind/neptune?style=social)| `Language` | 2025-05 | [![](https://img.shields.io/badge/arXiv-2505.00681-b31b1b.svg?style=plastic)](https://arxiv.org/abs/2505.00681) |
| VCR-Bench | [VCR-Bench: A Comprehensive Evaluation Framework for Video Chain-of-Thought Reasoning](https://arxiv.org/abs/2504.07956) | [GitHub](https://github.com/zhishuifeiqian/VCR-Bench) ![](https://img.shields.io/github/stars/zhishuifeiqian/VCR-Bench?style=social)<br>[Hugging_Face](https://huggingface.co/datasets/VLM-Reasoning/VCR-Bench) | `Language` |2025-04 | [![](https://img.shields.io/badge/arXiv-2504.07956-b31b1b.svg?style=plastic)](https://arxiv.org/abs/2504.07956) |
| SEED-Bench-R1 | [Exploring the Effect of Reinforcement Learning on Video Understanding: Insights from SEED-Bench-R1](https://arxiv.org/abs/2503.24376) | [GitHub](https://github.com/TencentARC/SEED-Bench-R1) ![](https://img.shields.io/github/stars/TencentARC/SEED-Bench-R1?style=social)<br>[Hugging_Face](https://huggingface.co/datasets/TencentARC/SEED-Bench-R1) | `Language` |2025-03 | [![](https://img.shields.io/badge/arXiv-2503.24376-b31b1b.svg?style=plastic)](https://arxiv.org/abs/2503.24376) |
| H2VU-Benchmark | [H2VU-Benchmark: A Comprehensive Benchmark for Hierarchical Holistic Video Understanding](https://arxiv.org/abs/2503.24008) | [GitHub](https://github.com/siriusrecco/H2VU-BenchMark) ![](https://img.shields.io/github/stars/siriusrecco/H2VU-BenchMark?style=social) | `Streaming` `Language` | 2025-03 | [![](https://img.shields.io/badge/arXiv-2503.24008-b31b1b.svg?style=plastic)](https://arxiv.org/abs/2503.24008) |
| OmniMMI | [OmniMMI: A Comprehensive Multi-modal Interaction Benchmark in Streaming Video Contexts](https://arxiv.org/abs/2503.22952) | [GitHub](https://github.com/OmniMMI/OmniMMI) ![](https://img.shields.io/github/stars/OmniMMI/OmniMMI?style=social)<br>[Hugging_Face](https://huggingface.co/datasets/ColorfulAI/OmniMMI) |`Streaming` `Language` |2025-03 | [![](https://img.shields.io/badge/CVPR-2025-blue.svg?style=plastic)]() |
| HAVEN | [Exploring Hallucination of Large Multimodal Models in Video Understanding: Benchmark, Analysis and Mitigation](https://arxiv.org/abs/2503.19622) | [GitHub](https://github.com/Hongcheng-Gao/HAVEN) ![](https://img.shields.io/github/stars/Hongcheng-Gao/HAVEN?style=social)<br>[Hugging_Face](https://github.com/Hongcheng-Gao/HAVEN/blob/main/Data/test_data.json) | `Language` |2025-03 | [![](https://img.shields.io/badge/arXiv-2503.19622-b31b1b.svg?style=plastic)](https://arxiv.org/abs/2503.19622) |
| V-STaR | [V-STaR: Benchmarking Video-LLMs on Video Spatio-Temporal Reasoning](https://arxiv.org/abs/2503.11495) | [GitHub](https://github.com/V-STaR-Bench/V-STaR) ![](https://img.shields.io/github/stars/V-STaR-Bench/V-STaR?style=social)<br>[Hugging_Face](https://huggingface.co/datasets/V-STaR-Bench/V-STaR) | `Language` |2025-03 | [![](https://img.shields.io/badge/arXiv-2503.11495-b31b1b.svg?style=plastic)](https://arxiv.org/abs/2503.11495) |
| COVER | [Reasoning is All You Need for Video Generalization](https://arxiv.org/abs/2503.10691) | [GitHub](https://github.com/gongyifan-hash/COVER-Benchmark) ![](https://img.shields.io/github/stars/gongyifan-hash/COVER-Benchmark?style=social) | `Language` | 2025-03 | [![](https://img.shields.io/badge/ACL-2025(Findings)-blue.svg?style=plastic)]() |
| MOMA-QA | [Towards Fine-Grained Video Question Answering](https://arxiv.org/abs/2503.06820) | - | `Language` | 2025-03 |  ![](https://img.shields.io/badge/arXiv-2503.06820-b31b1b.svg?style=plastic) |
| SVBench | [SVBench: A Benchmark with Temporal Multi-Turn Dialogues for Streaming Video Understanding](https://arxiv.org/abs/2502.10810) |  [GitHub](https://github.com/sotayang/SVBench) ![](https://img.shields.io/github/stars/sotayang/SVBench?style=social) | `Streaming` `Language` | 2025-02 | [![](https://img.shields.io/badge/ICLR-2025(Spotlight)-blue.svg?style=plastic)](https://openreview.net/forum?id=Hz4BYVY8YM) |
| StreamBench | [Streaming Video Understanding and Multi-round Interaction with Memory-enhanced Knowledge](https://arxiv.org/abs/2501.13468) | [GitHub](https://github.com/hmxiong/StreamChat) ![](https://img.shields.io/github/stars/hmxiong/StreamChat?style=social)<br>[Hugging_Face](https://huggingface.co/datasets/Barry-12138/StreamBench_v0.3) |`Streaming` `Language` |2025-01 | [![](https://img.shields.io/badge/ICLR-2025-blue.svg?style=plastic)]() |
| MMVU | [MMVU: Measuring Expert-Level Multi-Discipline Video Understanding](https://arxiv.org/abs/2501.12380) | [GitHub](https://github.com/yale-nlp/MMVU) ![](https://img.shields.io/github/stars/yale-nlp/MMVU?style=social)<br>[Hugging_Face](https://huggingface.co/datasets/yale-nlp/MMVU) | `Language` |2025-01 | ![](https://img.shields.io/badge/arXiv-2501.12380-b31b1b.svg?style=plastic) |
| OVO-Bench | [OVO-Bench: How Far is Your Video-LLMs from Real-World Online Video Understanding?](https://arxiv.org/abs/2501.05510) | [GitHub](https://github.com/JoeLeelyf/OVO-Bench) ![](https://img.shields.io/github/stars/JoeLeelyf/OVO-Bench?style=social)[Hugging_Face](https://huggingface.co/datasets/JoeLeelyf/OVO-Bench) |`Streaming` `Language` | 2025-01 | [![](https://img.shields.io/badge/CVPR-2025-blue.svg?style=plastic)]() |
| HLV-1K | [HLV-1K: A Large-scale Hour-Long Video Benchmark for Time-Specific Long Video Understanding](https://arxiv.org/abs/2501.01645) | [GitHub](https://github.com/Vincent-ZHQ/HLV-1K-Long-Video-Understanding-Benchmark) ![](https://img.shields.io/github/stars/Vincent-ZHQ/HLV-1K-Long-Video-Understanding-Benchmark?style=social) | `Language` | 2025-01 | [![](https://img.shields.io/badge/ICME-2025-blue.svg?style=plastic)]() |
| OVBench | [Online Video Understanding: OVBench and VideoChat-Online](https://arxiv.org/abs/2501.00584) | [GitHub](https://github.com/MCG-NJU/VideoChat-Online) ![](https://img.shields.io/github/stars/MCG-NJU/VideoChat-Online?style=social)<br>[Hugging_Face](https://huggingface.co/datasets/MCG-NJU/OVBench) |`Streaming` `Language` | 2025-01 | [![](https://img.shields.io/badge/CVPR-2025-blue.svg?style=plastic)]() |
| VSI-Bench | [Thinking in Space: How Multimodal Large Language Models See, Remember, and Recall Spaces](https://arxiv.org/abs/2412.14171) | [GitHub](https://github.com/vision-x-nyu/thinking-in-space) ![](https://img.shields.io/github/stars/vision-x-nyu/thinking-in-space?style=social)| `Language` | 2024-12 | [![](https://img.shields.io/badge/CVPR-2025(Oral)-blue.svg?style=plastic)]() |
| 3DSRBench | [3DSRBench: A Comprehensive 3D Spatial Reasoning Benchmark](https://arxiv.org/abs/2412.07825) | [Hugging_Face](https://huggingface.co/datasets/ccvl/3DSRBench) | `Language` | 2024-12 | [![](https://img.shields.io/badge/ICCV-2025-blue.svg?style=plastic)](https://iccv.thecvf.com/virtual/2025/poster/1723)  |
| BlackSwanSuite | [Black Swan: Abductive and Defeasible Video Reasoning in Unpredictable Events](https://arxiv.org/abs/2412.05725) | [GitHub](https://github.com/sahithyaravi/BlackSwan) ![](https://img.shields.io/github/stars/sahithyaravi/BlackSwan?style=social)<br>[Hugging_Face](https://huggingface.co/collections/UBC-ViL/black-swan-abductive-and-defeasible-reasoning-67de1a4ab7ddc22edf0b0542) | `Language`|2024-12 | [![](https://img.shields.io/badge/CVPR-2025-blue.svg?style=plastic)]() |
| TOMATO | [TOMATO: Assessing Visual Temporal Reasoning Capabilities in Multimodal Foundation Models](https://proceedings.iclr.cc/paper_files/paper/2025/hash/16ba99f25a235f1100a4014d71d34ad8-Abstract-Conference.html) | [Github](https://github.com/yale-nlp/TOMATO) ![](https://img.shields.io/github/stars/yale-nlp/TOMATO?style=social) | `Language` | 2024-10 | [![](https://img.shields.io/badge/CVPR-2025-blue.svg?style=plastic)](https://proceedings.iclr.cc/paper_files/paper/2025/hash/16ba99f25a235f1100a4014d71d34ad8-Abstract-Conference.html) |
| OmnixR    | [OmnixR: Evaluating Omni-modality Language Models on Reasoning across Modalities](https://proceedings.iclr.cc/paper_files/paper/2025/hash/aa3e67220ca4cd50010165c950fc8056-Abstract-Conference.html) | - | `Language` | 2024-10 |  [![](https://img.shields.io/badge/ICLR-2025-blue.svg?style=plastic)](https://proceedings.iclr.cc/paper_files/paper/2025/hash/aa3e67220ca4cd50010165c950fc8056-Abstract-Conference.html) |
| VideoVista | [VideoVista: A Versatile Benchmark for Video Understanding and Reasoning](https://arxiv.org/abs/2406.11303) | [Github](https://github.com/HITsz-TMG/Uni-MoE/tree/master/VideoVista) ![](https://img.shields.io/github/stars/HITsz-TMG/Uni-MoE?style=social) | `Language` | 2024-06 |  [![](https://img.shields.io/badge/arXiv-2406.11303-b31b1b.svg?style=plastic)](https://arxiv.org/abs/2406.11303) |
| SOK-Bench | [SOK-Bench: A Situated Video Reasoning Benchmark with Aligned Open-World Knowledge](https://arxiv.org/abs/2405.09713) | [GitHub](https://github.com/csbobby/SOK-Bench) ![](https://img.shields.io/github/stars/csbobby/SOK-Bench?style=social) | `Language` | 2024-05 |   [![](https://img.shields.io/badge/CVPR-2024-blue.svg?style=plastic)]() |
| CVRR-ES | [How Good is my Video LMM? Complex Video Reasoning and Robustness Evaluation Suite for Video-LMMs](https://arxiv.org/abs/2405.03690) |[GitHub](https://github.com/mbzuai-oryx/CVRR-Evaluation-Suite/) ![](https://img.shields.io/github/stars/mbzuai-oryx/CVRR-Evaluation-Suite?style=social) | `Language` | 2024-05 |  [![](https://img.shields.io/badge/arXiv-2405.03690-b31b1b.svg?style=plastic)](https://arxiv.org/abs/2405.03690) |


## ‚úà Related Survey

In addition, several recent and concurrent surveys have discussed multimodal or video reasoning. The works listed below offer complementary perspectives to ours, reflecting the field‚Äôs rapid and parallel development:
- [Awesome-Video-Reasoning](https://github.com/Video-Reaso`N/A`wesome-Video-Reasoning)
- [Awesome-LLMs-for-Video-Understanding](https://github.com/yunlong10/Awesome-LLMs-for-Video-Understanding)
- [Awesome-Multimodal-Spatial-Reasoning](https://github.com/zhengxuJosh/Awesome-Multimodal-Spatial-Reasoning)
- [Awesome-Streaming-Video-Understanding](https://github.com/sotayang/Awesome-Streaming-Video-Understanding)
- [Awesome-Video-LMM-Post-Training](https://github.com/yunlong10/Awesome-Video-LMM-Post-Training)
---
- [Awesome-World-Models](https://github.com/knightnemo/Awesome-World-Models)
- [Awesome-World-Models-for-Robotics](https://github.com/leofan90/Awesome-World-Models)
---
- [Awesome-MCoT](https://github.com/yaotingwangofficial/Awesome-MCoT)
- [Awesome-Latent-CoT](https://github.com/EIT-NLP/Awesome-Latent-CoT)
- [Awesome-Latent-Space](https://github.com/YU-deep/Awesome-Latent-Space)


## üåü Star History

[![Star History Chart](https://api.star-history.com/svg?repos=LJungang/Awesome-Video-Reasoning-Landscape&type=Date)](https://star-history.com/#LJungang/Awesome-Omni-Large-Models-and-Datasets&Date)

## ‚ô•Ô∏è Contributors

<!--
<a href="https://github.com/LJungang/Awesome-Video-Reasoning-Landscape/graphs/contributors">
  <img src="https://contrib.rocks/image?repo=LJungang/Awesome-Video-Reasoning-Landscape" />
</a>
 -->

<a href="https://github.comLJungang/Awesome-Video-Reasoning-Landscape/graphs/contributors">
  <img src="https://contrib.rocks/image?repo=LJungang/Awesome-Video-Reasoning-Landscape" alt="Contributors for Awesome Video Reasoning Landscape"/>
</a>

<!-- markdownlint-enable MD033 -->
